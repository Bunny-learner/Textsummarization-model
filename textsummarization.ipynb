{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_JTS4k4agLI",
        "outputId": "3391fe84-0617-456f-dda9-d3ed2ef81c17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
        "train_data = dataset['train'].select(range(3000))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "R1Oqn3Y-icQF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc73dc8e-cd7b-4ead-ffec-d3dfe4a60031"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data[0]['article'])\n",
        "print(train_data[0]['highlights'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GynHnaawa2MV",
        "outputId": "1cf771bf-d6c2-4c66-ba8c-987be5913f7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" he told an Australian interviewer earlier this month. \"I don't think I'll be particularly extravagant. \"The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\" At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film \"Hostel: Part II,\" currently six places below his number one movie on the UK box office chart. Details of how he'll mark his landmark birthday are under wraps. His agent and publicist had no comment on his plans. \"I'll definitely have some sort of party,\" he said in an interview. \"Hopefully none of you will be reading about it.\" Radcliffe's earnings from the first five Potter films have been held in a trust fund which he has not been able to touch. Despite his growing fame and riches, the actor says he is keeping his feet firmly on the ground. \"People are always looking to say 'kid star goes off the rails,'\" he told reporters last month. \"But I try very hard not to go that way because it would be too easy for them.\" His latest outing as the boy wizard in \"Harry Potter and the Order of the Phoenix\" is breaking records on both sides of the Atlantic and he will reprise the role in the last two films.  Watch I-Reporter give her review of Potter's latest » . There is life beyond Potter, however. The Londoner has filmed a TV movie called \"My Boy Jack,\" about author Rudyard Kipling and his son, due for release later this year. He will also appear in \"December Boys,\" an Australian film about four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured teenager in Peter Shaffer's \"Equus.\" Meanwhile, he is braced for even closer media scrutiny now that he's legally an adult: \"I just think I'm going to be more sort of fair game,\" he told Reuters. E-mail to a friend . Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, or redistributed.\n",
            "Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday .\n",
            "Young actor says he has no plans to fritter his cash away .\n",
            "Radcliffe's earnings from first five Potter films have been held in trust fund .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "def cleantext(text):\n",
        "\n",
        "  text=text.lower()\n",
        "  text = re.sub(r'[^a-zA-Z\\s]', '', text)#removing special characters and digits\n",
        "  tokens=word_tokenize(text)\n",
        "  return tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6dAxZ0ObAnu",
        "outputId": "aa70be19-4c77-4b27-a495-0497b7344040"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "train_dataset=train_data.to_pandas()\n",
        "X_train=train_dataset['article']\n",
        "y_train=train_dataset['highlights']\n",
        "tokenized_docs=[cleantext(doc) for doc in X_train]\n",
        "tokenized_summaries=[cleantext(s) for s in y_train]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYCtFzfxds6T",
        "outputId": "3dc5eb10-fc47-4119-d4c6-e404a5948298"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_docs[:2])\n",
        "print(tokenized_summaries[:2])"
      ],
      "metadata": {
        "id": "MPO00BR1er-V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02eedeb7-e565-49c3-e324-5bc1bff271c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['london', 'england', 'reuters', 'harry', 'potter', 'star', 'daniel', 'radcliffe', 'gains', 'access', 'to', 'a', 'reported', 'million', 'million', 'fortune', 'as', 'he', 'turns', 'on', 'monday', 'but', 'he', 'insists', 'the', 'money', 'wont', 'cast', 'a', 'spell', 'on', 'him', 'daniel', 'radcliffe', 'as', 'harry', 'potter', 'in', 'harry', 'potter', 'and', 'the', 'order', 'of', 'the', 'phoenix', 'to', 'the', 'disappointment', 'of', 'gossip', 'columnists', 'around', 'the', 'world', 'the', 'young', 'actor', 'says', 'he', 'has', 'no', 'plans', 'to', 'fritter', 'his', 'cash', 'away', 'on', 'fast', 'cars', 'drink', 'and', 'celebrity', 'parties', 'i', 'dont', 'plan', 'to', 'be', 'one', 'of', 'those', 'people', 'who', 'as', 'soon', 'as', 'they', 'turn', 'suddenly', 'buy', 'themselves', 'a', 'massive', 'sports', 'car', 'collection', 'or', 'something', 'similar', 'he', 'told', 'an', 'australian', 'interviewer', 'earlier', 'this', 'month', 'i', 'dont', 'think', 'ill', 'be', 'particularly', 'extravagant', 'the', 'things', 'i', 'like', 'buying', 'are', 'things', 'that', 'cost', 'about', 'pounds', 'books', 'and', 'cds', 'and', 'dvds', 'at', 'radcliffe', 'will', 'be', 'able', 'to', 'gamble', 'in', 'a', 'casino', 'buy', 'a', 'drink', 'in', 'a', 'pub', 'or', 'see', 'the', 'horror', 'film', 'hostel', 'part', 'ii', 'currently', 'six', 'places', 'below', 'his', 'number', 'one', 'movie', 'on', 'the', 'uk', 'box', 'office', 'chart', 'details', 'of', 'how', 'hell', 'mark', 'his', 'landmark', 'birthday', 'are', 'under', 'wraps', 'his', 'agent', 'and', 'publicist', 'had', 'no', 'comment', 'on', 'his', 'plans', 'ill', 'definitely', 'have', 'some', 'sort', 'of', 'party', 'he', 'said', 'in', 'an', 'interview', 'hopefully', 'none', 'of', 'you', 'will', 'be', 'reading', 'about', 'it', 'radcliffes', 'earnings', 'from', 'the', 'first', 'five', 'potter', 'films', 'have', 'been', 'held', 'in', 'a', 'trust', 'fund', 'which', 'he', 'has', 'not', 'been', 'able', 'to', 'touch', 'despite', 'his', 'growing', 'fame', 'and', 'riches', 'the', 'actor', 'says', 'he', 'is', 'keeping', 'his', 'feet', 'firmly', 'on', 'the', 'ground', 'people', 'are', 'always', 'looking', 'to', 'say', 'kid', 'star', 'goes', 'off', 'the', 'rails', 'he', 'told', 'reporters', 'last', 'month', 'but', 'i', 'try', 'very', 'hard', 'not', 'to', 'go', 'that', 'way', 'because', 'it', 'would', 'be', 'too', 'easy', 'for', 'them', 'his', 'latest', 'outing', 'as', 'the', 'boy', 'wizard', 'in', 'harry', 'potter', 'and', 'the', 'order', 'of', 'the', 'phoenix', 'is', 'breaking', 'records', 'on', 'both', 'sides', 'of', 'the', 'atlantic', 'and', 'he', 'will', 'reprise', 'the', 'role', 'in', 'the', 'last', 'two', 'films', 'watch', 'ireporter', 'give', 'her', 'review', 'of', 'potters', 'latest', 'there', 'is', 'life', 'beyond', 'potter', 'however', 'the', 'londoner', 'has', 'filmed', 'a', 'tv', 'movie', 'called', 'my', 'boy', 'jack', 'about', 'author', 'rudyard', 'kipling', 'and', 'his', 'son', 'due', 'for', 'release', 'later', 'this', 'year', 'he', 'will', 'also', 'appear', 'in', 'december', 'boys', 'an', 'australian', 'film', 'about', 'four', 'boys', 'who', 'escape', 'an', 'orphanage', 'earlier', 'this', 'year', 'he', 'made', 'his', 'stage', 'debut', 'playing', 'a', 'tortured', 'teenager', 'in', 'peter', 'shaffers', 'equus', 'meanwhile', 'he', 'is', 'braced', 'for', 'even', 'closer', 'media', 'scrutiny', 'now', 'that', 'hes', 'legally', 'an', 'adult', 'i', 'just', 'think', 'im', 'going', 'to', 'be', 'more', 'sort', 'of', 'fair', 'game', 'he', 'told', 'reuters', 'email', 'to', 'a', 'friend', 'copyright', 'reuters', 'all', 'rights', 'reservedthis', 'material', 'may', 'not', 'be', 'published', 'broadcast', 'rewritten', 'or', 'redistributed'], ['editors', 'note', 'in', 'our', 'behind', 'the', 'scenes', 'series', 'cnn', 'correspondents', 'share', 'their', 'experiences', 'in', 'covering', 'news', 'and', 'analyze', 'the', 'stories', 'behind', 'the', 'events', 'here', 'soledad', 'obrien', 'takes', 'users', 'inside', 'a', 'jail', 'where', 'many', 'of', 'the', 'inmates', 'are', 'mentally', 'ill', 'an', 'inmate', 'housed', 'on', 'the', 'forgotten', 'floor', 'where', 'many', 'mentally', 'ill', 'inmates', 'are', 'housed', 'in', 'miami', 'before', 'trial', 'miami', 'florida', 'cnn', 'the', 'ninth', 'floor', 'of', 'the', 'miamidade', 'pretrial', 'detention', 'facility', 'is', 'dubbed', 'the', 'forgotten', 'floor', 'here', 'inmates', 'with', 'the', 'most', 'severe', 'mental', 'illnesses', 'are', 'incarcerated', 'until', 'theyre', 'ready', 'to', 'appear', 'in', 'court', 'most', 'often', 'they', 'face', 'drug', 'charges', 'or', 'charges', 'of', 'assaulting', 'an', 'officer', 'charges', 'that', 'judge', 'steven', 'leifman', 'says', 'are', 'usually', 'avoidable', 'felonies', 'he', 'says', 'the', 'arrests', 'often', 'result', 'from', 'confrontations', 'with', 'police', 'mentally', 'ill', 'people', 'often', 'wont', 'do', 'what', 'theyre', 'told', 'when', 'police', 'arrive', 'on', 'the', 'scene', 'confrontation', 'seems', 'to', 'exacerbate', 'their', 'illness', 'and', 'they', 'become', 'more', 'paranoid', 'delusional', 'and', 'less', 'likely', 'to', 'follow', 'directions', 'according', 'to', 'leifman', 'so', 'they', 'end', 'up', 'on', 'the', 'ninth', 'floor', 'severely', 'mentally', 'disturbed', 'but', 'not', 'getting', 'any', 'real', 'help', 'because', 'theyre', 'in', 'jail', 'we', 'toured', 'the', 'jail', 'with', 'leifman', 'he', 'is', 'well', 'known', 'in', 'miami', 'as', 'an', 'advocate', 'for', 'justice', 'and', 'the', 'mentally', 'ill', 'even', 'though', 'we', 'were', 'not', 'exactly', 'welcomed', 'with', 'open', 'arms', 'by', 'the', 'guards', 'we', 'were', 'given', 'permission', 'to', 'shoot', 'videotape', 'and', 'tour', 'the', 'floor', 'go', 'inside', 'the', 'forgotten', 'floor', 'at', 'first', 'its', 'hard', 'to', 'determine', 'where', 'the', 'people', 'are', 'the', 'prisoners', 'are', 'wearing', 'sleeveless', 'robes', 'imagine', 'cutting', 'holes', 'for', 'arms', 'and', 'feet', 'in', 'a', 'heavy', 'wool', 'sleeping', 'bag', 'thats', 'kind', 'of', 'what', 'they', 'look', 'like', 'theyre', 'designed', 'to', 'keep', 'the', 'mentally', 'ill', 'patients', 'from', 'injuring', 'themselves', 'thats', 'also', 'why', 'they', 'have', 'no', 'shoes', 'laces', 'or', 'mattresses', 'leifman', 'says', 'about', 'onethird', 'of', 'all', 'people', 'in', 'miamidade', 'county', 'jails', 'are', 'mentally', 'ill', 'so', 'he', 'says', 'the', 'sheer', 'volume', 'is', 'overwhelming', 'the', 'system', 'and', 'the', 'result', 'is', 'what', 'we', 'see', 'on', 'the', 'ninth', 'floor', 'of', 'course', 'it', 'is', 'a', 'jail', 'so', 'its', 'not', 'supposed', 'to', 'be', 'warm', 'and', 'comforting', 'but', 'the', 'lights', 'glare', 'the', 'cells', 'are', 'tiny', 'and', 'its', 'loud', 'we', 'see', 'two', 'sometimes', 'three', 'men', 'sometimes', 'in', 'the', 'robes', 'sometimes', 'naked', 'lying', 'or', 'sitting', 'in', 'their', 'cells', 'i', 'am', 'the', 'son', 'of', 'the', 'president', 'you', 'need', 'to', 'get', 'me', 'out', 'of', 'here', 'one', 'man', 'shouts', 'at', 'me', 'he', 'is', 'absolutely', 'serious', 'convinced', 'that', 'help', 'is', 'on', 'the', 'way', 'if', 'only', 'he', 'could', 'reach', 'the', 'white', 'house', 'leifman', 'tells', 'me', 'that', 'these', 'prisonerpatients', 'will', 'often', 'circulate', 'through', 'the', 'system', 'occasionally', 'stabilizing', 'in', 'a', 'mental', 'hospital', 'only', 'to', 'return', 'to', 'jail', 'to', 'face', 'their', 'charges', 'its', 'brutally', 'unjust', 'in', 'his', 'mind', 'and', 'he', 'has', 'become', 'a', 'strong', 'advocate', 'for', 'changing', 'things', 'in', 'miami', 'over', 'a', 'meal', 'later', 'we', 'talk', 'about', 'how', 'things', 'got', 'this', 'way', 'for', 'mental', 'patients', 'leifman', 'says', 'years', 'ago', 'people', 'were', 'considered', 'lunatics', 'and', 'they', 'were', 'locked', 'up', 'in', 'jails', 'even', 'if', 'they', 'had', 'no', 'charges', 'against', 'them', 'they', 'were', 'just', 'considered', 'unfit', 'to', 'be', 'in', 'society', 'over', 'the', 'years', 'he', 'says', 'there', 'was', 'some', 'public', 'outcry', 'and', 'the', 'mentally', 'ill', 'were', 'moved', 'out', 'of', 'jails', 'and', 'into', 'hospitals', 'but', 'leifman', 'says', 'many', 'of', 'these', 'mental', 'hospitals', 'were', 'so', 'horrible', 'they', 'were', 'shut', 'down', 'where', 'did', 'the', 'patients', 'go', 'nowhere', 'the', 'streets', 'they', 'became', 'in', 'many', 'cases', 'the', 'homeless', 'he', 'says', 'they', 'never', 'got', 'treatment', 'leifman', 'says', 'in', 'there', 'were', 'more', 'than', 'half', 'a', 'million', 'people', 'in', 'state', 'mental', 'hospitals', 'and', 'today', 'that', 'number', 'has', 'been', 'reduced', 'percent', 'and', 'to', 'people', 'are', 'in', 'mental', 'hospitals', 'the', 'judge', 'says', 'hes', 'working', 'to', 'change', 'this', 'starting', 'in', 'many', 'inmates', 'who', 'would', 'otherwise', 'have', 'been', 'brought', 'to', 'the', 'forgotten', 'floor', 'will', 'instead', 'be', 'sent', 'to', 'a', 'new', 'mental', 'health', 'facility', 'the', 'first', 'step', 'on', 'a', 'journey', 'toward', 'longterm', 'treatment', 'not', 'just', 'punishment', 'leifman', 'says', 'its', 'not', 'the', 'complete', 'answer', 'but', 'its', 'a', 'start', 'leifman', 'says', 'the', 'best', 'part', 'is', 'that', 'its', 'a', 'winwin', 'solution', 'the', 'patients', 'win', 'the', 'families', 'are', 'relieved', 'and', 'the', 'state', 'saves', 'money', 'by', 'simply', 'not', 'cycling', 'these', 'prisoners', 'through', 'again', 'and', 'again', 'and', 'for', 'leifman', 'justice', 'is', 'served', 'email', 'to', 'a', 'friend']]\n",
            "[['harry', 'potter', 'star', 'daniel', 'radcliffe', 'gets', 'm', 'fortune', 'as', 'he', 'turns', 'monday', 'young', 'actor', 'says', 'he', 'has', 'no', 'plans', 'to', 'fritter', 'his', 'cash', 'away', 'radcliffes', 'earnings', 'from', 'first', 'five', 'potter', 'films', 'have', 'been', 'held', 'in', 'trust', 'fund'], ['mentally', 'ill', 'inmates', 'in', 'miami', 'are', 'housed', 'on', 'the', 'forgotten', 'floor', 'judge', 'steven', 'leifman', 'says', 'most', 'are', 'there', 'as', 'a', 'result', 'of', 'avoidable', 'felonies', 'while', 'cnn', 'tours', 'facility', 'patient', 'shouts', 'i', 'am', 'the', 'son', 'of', 'the', 'president', 'leifman', 'says', 'the', 'system', 'is', 'unjust', 'and', 'hes', 'fighting', 'for', 'change']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the GloVe vectors (you can choose other versions as well)\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "metadata": {
        "id": "TgZ4tbS6GYbV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b74c59c0-386b-4b38-ea6d-097be11a60b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-12 14:55:53--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2025-05-12 14:55:53--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2025-05-12 14:55:53--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.2’\n",
            "\n",
            "glove.6B.zip.2      100%[===================>] 822.24M  5.01MB/s    in 2m 39s  \n",
            "\n",
            "2025-05-12 14:58:33 (5.16 MB/s) - ‘glove.6B.zip.2’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace glove.6B.100d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace glove.6B.200d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace glove.6B.300d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy\n",
        "import numpy as np\n",
        "\n",
        "# Load GloVe embeddings\n",
        "def load_glove_embeddings(glove_file_path):\n",
        "    embeddings = {}\n",
        "    with open(glove_file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.array(values[1:], dtype='float32')\n",
        "            embeddings[word] = vector\n",
        "    return embeddings\n",
        "\n",
        "# Convert a tokenized document to GloVe vectors\n",
        "def convert_to_vectors(tokens, glove_embeddings):\n",
        "    vectors = []\n",
        "    for token in tokens:\n",
        "        if token in glove_embeddings:\n",
        "            vectors.append(glove_embeddings[token])\n",
        "        else:\n",
        "            vectors.append(np.zeros(100))  # Use zero vector for unknown words\n",
        "    return vectors\n",
        "\n",
        "\n",
        "# Load GloVe 100d vectors (replace path with your downloaded file)\n",
        "glove_file_path = 'glove.6B.100d.txt'\n",
        "glove_embeddings = load_glove_embeddings(glove_file_path)\n",
        "\n",
        "# Convert tokenized documents into GloVe vectors\n",
        "vectorized_docs = [convert_to_vectors(doc, glove_embeddings) for doc in tokenized_docs]\n",
        "vectorized_summaries = [convert_to_vectors(s, glove_embeddings) for s in tokenized_summaries]\n",
        "\n",
        "# Display the result\n",
        "print(np.array(vectorized_docs[0]))  # Check the vectors for the first tokenized document\n"
      ],
      "metadata": {
        "id": "QzGlvlP6sJEC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c4a1040-81d3-4488-d7ba-f3c21abc8a33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "[[ 0.60553002 -0.050886   -0.15460999 ...  0.41466999  0.34641001\n",
            "   0.019886  ]\n",
            " [-0.092902    0.43667999  0.63845998 ...  0.68544     0.28566\n",
            "  -0.40700001]\n",
            " [-0.37518001 -0.75599998 -0.15465    ... -0.053903    0.37886\n",
            "   0.35159999]\n",
            " ...\n",
            " [-0.22145     0.20658     0.51225001 ... -0.043929   -0.67294002\n",
            "   0.11487   ]\n",
            " [ 0.31039     0.64859003  0.28481001 ... -0.88554001  0.91767001\n",
            "  -0.57252997]\n",
            " [-0.56418997 -0.69006997  0.64247    ...  0.61062002 -0.20618001\n",
            "  -0.035852  ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, TimeDistributed\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n"
      ],
      "metadata": {
        "id": "4jCk_m1fvy4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "padded_docs = pad_sequences(vectorized_docs, maxlen=400, padding='post', truncating='post')\n",
        "padded_summaries = pad_sequences(vectorized_summaries, maxlen=100, padding='post', truncating='post')\n",
        "\n",
        "print(padded_docs.shape, padded_summaries.shape)  # Check the shape of padded sequences\n"
      ],
      "metadata": {
        "id": "QCbnOrbjwCwx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "261e0d34-54ce-4945-c675-69d7b88d40cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3000, 400, 100) (3000, 100, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Combine tokenized documents and summaries to create a vocabulary\n",
        "all_tokens = [word for doc in tokenized_docs + tokenized_summaries for word in doc]\n",
        "word_counts = Counter(all_tokens)\n",
        "\n",
        "max_vocab_size = 5000  # Limiting vocab size to 10,000 most frequent words\n",
        "word2idx = {word: idx for idx, (word, _) in enumerate(word_counts.most_common(max_vocab_size))}\n",
        "\n",
        "\n",
        "# Add a padding token\n",
        "word2idx['<PAD>'] = 0\n",
        "\n",
        "# Display the word2idx mapping (optional)\n",
        "print(f\"Word to index mapping: {list(word2idx.items())[:10]}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "AlPEtwCIwkpY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00b57c7c-2cb0-473c-c692-7a0ed33e2f8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word to index mapping: [('the', 0), ('to', 1), ('of', 2), ('a', 3), ('and', 4), ('in', 5), ('said', 6), ('that', 7), ('for', 8), ('is', 9)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the embedding matrix with zeros\n",
        "embedding_dim = 100\n",
        "embedding_matrix = np.zeros((len(word2idx), embedding_dim))\n",
        "\n",
        "# Fill the embedding matrix with GloVe vectors\n",
        "for word, idx in word2idx.items():\n",
        "    if word in glove_embeddings:\n",
        "        embedding_matrix[idx] = glove_embeddings[word]\n",
        "    else:\n",
        "        embedding_matrix[idx] = np.zeros(embedding_dim)\n",
        "\n",
        "\n",
        "print(embedding_matrix[:5])\n"
      ],
      "metadata": {
        "id": "JgtHU9TLxd83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "883c61e4-b6d6-4a4d-a4b8-b4b91ec3cc40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [-1.89700007e-01  5.00239991e-02  1.90840006e-01 -4.91839983e-02\n",
            "  -8.97369981e-02  2.10060000e-01 -5.49520016e-01  9.83769968e-02\n",
            "  -2.01350003e-01  3.42409998e-01 -9.26769972e-02  1.60999998e-01\n",
            "  -1.32679999e-01 -2.81599998e-01  1.87370002e-01 -4.29589987e-01\n",
            "   9.60389972e-01  1.39719993e-01 -1.07809997e+00  4.05180007e-01\n",
            "   5.05389988e-01 -5.50639987e-01  4.84400004e-01  3.80439997e-01\n",
            "  -2.90549989e-03 -3.49420011e-01 -9.96960029e-02 -7.83680022e-01\n",
            "   1.03629994e+00 -2.31399998e-01 -4.71210003e-01  5.71259975e-01\n",
            "  -2.14540005e-01  3.59580010e-01 -4.83190000e-01  1.08749998e+00\n",
            "   2.85239995e-01  1.24470003e-01 -3.92480008e-02 -7.67320022e-02\n",
            "  -7.63429999e-01 -3.24090004e-01 -5.74899971e-01 -1.08930004e+00\n",
            "  -4.18110013e-01  4.51200008e-01  1.21119998e-01 -5.13670027e-01\n",
            "  -1.33489996e-01 -1.13779998e+00 -2.87680000e-01  1.67740002e-01\n",
            "   5.58040023e-01  1.53869998e+00  1.88590009e-02 -2.97210002e+00\n",
            "  -2.42160007e-01 -9.24950004e-01  2.19919991e+00  2.82339990e-01\n",
            "  -3.47799987e-01  5.16210020e-01 -4.33869988e-01  3.68519992e-01\n",
            "   7.45729983e-01  7.21020028e-02  2.79309988e-01  9.25689995e-01\n",
            "  -5.03359996e-02 -8.58560026e-01 -1.35800004e-01 -9.25509989e-01\n",
            "  -3.39910001e-01 -1.03939998e+00 -6.72030002e-02 -2.13789999e-01\n",
            "  -4.76900011e-01  2.13770002e-01 -8.40080023e-01  5.25359996e-02\n",
            "   5.92980027e-01  2.96039999e-01 -6.76440001e-01  1.39160007e-01\n",
            "  -1.55040002e+00 -2.07650006e-01  7.22199976e-01  5.20560026e-01\n",
            "  -7.62209967e-02 -1.51940003e-01 -1.31339997e-01  5.86169995e-02\n",
            "  -3.18690002e-01 -6.14189982e-01 -6.23929977e-01 -4.15479988e-01\n",
            "  -3.81750017e-02 -3.98039997e-01  4.76469994e-01 -1.59830004e-01]\n",
            " [-1.52899995e-01 -2.42789999e-01  8.98370028e-01  1.69960007e-01\n",
            "   5.35160005e-01  4.87839997e-01 -5.88259995e-01 -1.79820001e-01\n",
            "  -1.35810006e+00  4.25410002e-01  1.53770000e-01  2.42149994e-01\n",
            "   1.34739995e-01  4.11929995e-01  6.70430005e-01 -5.64180017e-01\n",
            "   4.29850012e-01 -1.21830003e-02 -1.16769999e-01  3.17809999e-01\n",
            "   5.41770011e-02 -5.42730018e-02  3.55159998e-01 -3.02410007e-01\n",
            "   3.14339995e-01 -3.38459998e-01  7.17149973e-01 -2.68550009e-01\n",
            "  -1.58370003e-01 -4.74669993e-01  5.15809990e-02 -3.32520008e-01\n",
            "   1.50030002e-01 -1.29899994e-01 -5.46169996e-01 -3.78430009e-01\n",
            "   6.42610013e-01  8.21870029e-01 -8.00060034e-02  7.84789994e-02\n",
            "  -9.69760001e-01 -5.77409983e-01  5.64909995e-01 -3.98730010e-01\n",
            "  -5.70989996e-02  1.97430000e-01  6.57059997e-02 -4.80919987e-01\n",
            "  -2.01250002e-01 -4.08340007e-01  3.94560009e-01 -2.64199991e-02\n",
            "  -1.18380003e-01  1.01199996e+00 -5.31710029e-01 -2.74740005e+00\n",
            "  -4.29809988e-02 -7.48489976e-01  1.75740004e+00  5.90849996e-01\n",
            "   4.88499999e-02  7.82670021e-01  3.84970009e-01  4.20969993e-01\n",
            "   6.78820014e-01  1.03370003e-01  6.32799983e-01 -2.65950002e-02\n",
            "   5.86470008e-01 -4.43320006e-01  3.30570012e-01 -1.20219998e-01\n",
            "  -5.56450009e-01  7.36109987e-02  2.09150001e-01  4.33950007e-01\n",
            "  -1.27609996e-02  8.98739994e-02 -1.79910004e+00  8.48079994e-02\n",
            "   7.71120012e-01  6.31049991e-01 -9.06849980e-01  6.03259981e-01\n",
            "  -1.75150001e+00  1.85959995e-01 -5.06869972e-01 -7.02030003e-01\n",
            "   6.65780008e-01 -8.13040018e-01  1.87120005e-01 -1.84879992e-02\n",
            "  -2.67569989e-01  7.26999998e-01 -5.93630016e-01 -3.48390013e-01\n",
            "  -5.60940027e-01 -5.91000021e-01  1.00390005e+00  2.06640005e-01]\n",
            " [-2.70859987e-01  4.40060012e-02 -2.02600006e-02 -1.73950002e-01\n",
            "   6.44400001e-01  7.12130010e-01  3.55100006e-01  4.71379995e-01\n",
            "  -2.96370000e-01  5.44269979e-01 -7.22940028e-01 -4.76119993e-03\n",
            "   4.06109989e-02  4.32359986e-02  2.97289997e-01  1.07249998e-01\n",
            "   4.01560009e-01 -5.36620021e-01  3.33819985e-02  6.73960000e-02\n",
            "   6.45560026e-01 -8.55230018e-02  1.41029999e-01  9.45390016e-02\n",
            "   7.49469995e-01 -1.94000006e-01 -6.87390029e-01 -4.17409986e-01\n",
            "  -2.28070006e-01  1.19999997e-01 -4.89989996e-01  8.09449971e-01\n",
            "   4.51380014e-02 -1.18979998e-01  2.01609999e-01  3.92760009e-01\n",
            "  -2.01210007e-01  3.13540012e-01  7.53040016e-01  2.59070009e-01\n",
            "  -1.15659997e-01 -2.93189995e-02  9.34989989e-01 -3.60670000e-01\n",
            "   5.24200022e-01  2.37059996e-01  5.27149975e-01  2.28689998e-01\n",
            "  -5.19580007e-01 -7.93489993e-01 -2.03679994e-01 -5.01869977e-01\n",
            "   1.87480003e-01  9.42820013e-01 -4.48339999e-01 -3.67919993e+00\n",
            "   4.41830009e-02 -2.67509997e-01  2.19970012e+00  2.40999997e-01\n",
            "  -3.34249996e-02  6.95529997e-01 -6.44720018e-01 -7.22770020e-03\n",
            "   8.95749986e-01  2.00149998e-01  4.64929998e-01  6.19329989e-01\n",
            "  -1.06600001e-01  8.69100019e-02 -4.62300003e-01  1.82620004e-01\n",
            "  -1.58490002e-01  2.07909998e-02  1.93729997e-01  6.34260029e-02\n",
            "  -3.16729993e-01 -4.81770009e-01 -1.38479996e+00  1.36690006e-01\n",
            "   9.68590021e-01  4.99650016e-02 -2.73799986e-01 -3.56860012e-02\n",
            "  -1.05770004e+00 -2.44670004e-01  9.03659999e-01 -1.24420002e-01\n",
            "   8.07759985e-02 -8.34010005e-01  5.72009981e-01  8.89450014e-02\n",
            "  -4.25319999e-01 -1.82530005e-02 -7.99949989e-02 -2.85809994e-01\n",
            "  -1.08899996e-02 -4.92300004e-01  6.36870027e-01  2.36420006e-01]\n",
            " [-7.19529986e-02  2.31270000e-01  2.37310007e-02 -5.06380022e-01\n",
            "   3.39230001e-01  1.95899993e-01 -3.29430014e-01  1.83640003e-01\n",
            "  -1.80570006e-01  2.89629996e-01  2.04480007e-01 -5.49600005e-01\n",
            "   2.73990005e-01  5.83270013e-01  2.04679996e-01 -4.92280006e-01\n",
            "   1.99739993e-01 -7.02370033e-02 -8.80490005e-01  2.94849992e-01\n",
            "   1.40709996e-01 -1.00900002e-01  9.94490027e-01  3.69729996e-01\n",
            "   4.45540011e-01  2.89979994e-01 -1.37600005e-01 -5.63650012e-01\n",
            "  -2.93649994e-02 -4.12200004e-01 -2.52689987e-01  6.31810009e-01\n",
            "  -4.47670013e-01  2.43630007e-01 -1.08130001e-01  2.51639992e-01\n",
            "   4.69669998e-01  3.75499994e-01 -2.36129999e-01 -1.41289994e-01\n",
            "  -4.45369989e-01 -6.57369971e-01 -4.24209982e-02 -2.86359996e-01\n",
            "  -2.88109988e-01  6.37660027e-02  2.02810004e-01 -5.35420001e-01\n",
            "   4.13069993e-01 -5.97220004e-01 -3.86139989e-01  1.93890005e-01\n",
            "  -1.78090006e-01  1.66180003e+00 -1.18190004e-02 -2.37369990e+00\n",
            "   5.84269986e-02 -2.69800007e-01  1.28230000e+00  8.19249988e-01\n",
            "  -2.23220006e-01  7.29319990e-01 -5.32109998e-02  4.35070008e-01\n",
            "   8.50109994e-01 -4.29349989e-01  9.26639974e-01  3.90509993e-01\n",
            "   1.05850005e+00 -2.45609999e-01 -1.82650000e-01 -5.32800019e-01\n",
            "   5.95179982e-02 -6.60189986e-01  1.89909995e-01  2.88360000e-01\n",
            "  -2.43399993e-01  5.27840018e-01 -6.57620013e-01 -1.40809998e-01\n",
            "   1.04910004e+00  5.13400018e-01 -2.38159999e-01  6.98949993e-01\n",
            "  -1.48130000e+00 -2.48699993e-01 -1.79360002e-01 -5.91370016e-02\n",
            "  -8.05599988e-02 -4.87819999e-01  1.44870002e-02 -6.25899971e-01\n",
            "  -3.23670000e-01  4.18619990e-01 -1.08070004e+00  4.67420012e-01\n",
            "  -4.99309987e-01 -7.18949974e-01  8.68939996e-01  1.95390001e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# dimensions\n",
        "embedding_dim = 100\n",
        "latent_dim = 256\n",
        "vocab_size = 5000\n",
        "max_input_len = 400\n",
        "max_summary_len =100\n",
        "\n",
        "#encoder part\n",
        "encoder_inputs = Input(shape=(max_input_len,))\n",
        "enc_emb = Embedding(input_dim=vocab_size, output_dim=embedding_dim,\n",
        "                    input_length=max_input_len)(encoder_inputs)\n",
        "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# decoder part\n",
        "decoder_inputs = Input(shape=(max_summary_len,))\n",
        "dec_emb = Embedding(input_dim=vocab_size+1 , output_dim=embedding_dim,\n",
        "                    weights=[embedding_matrix], input_length=max_summary_len,\n",
        "                    trainable=False)(decoder_inputs)\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
        "\n",
        "decoder_dense = Dense(vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "#model details\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "ZE8U5-1KwWS9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "outputId": "05d9d6cf-4326-41d6-cba2-9c580bb5b1ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m400\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_3       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_2         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m400\u001b[0m, \u001b[38;5;34m100\u001b[0m)  │    \u001b[38;5;34m500,000\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_3         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m)  │    \u001b[38;5;34m500,100\u001b[0m │ input_layer_3[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),     │    \u001b[38;5;34m365,568\u001b[0m │ embedding_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │            │                   │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m,      │    \u001b[38;5;34m365,568\u001b[0m │ embedding_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
              "│                     │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ lstm_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],     │\n",
              "│                     │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ lstm_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]      │\n",
              "│                     │ \u001b[38;5;34m256\u001b[0m)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m5000\u001b[0m) │  \u001b[38;5;34m1,285,000\u001b[0m │ lstm_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_3       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_2         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">500,000</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_3         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">500,100</span> │ input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">365,568</span> │ embedding_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │                   │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>,      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">365,568</span> │ embedding_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ lstm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],     │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ lstm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]      │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5000</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,285,000</span> │ lstm_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,016,236\u001b[0m (11.51 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,016,236</span> (11.51 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,516,136\u001b[0m (9.60 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,516,136</span> (9.60 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m500,100\u001b[0m (1.91 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">500,100</span> (1.91 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Prepare the input data for the encoder and decoder\n",
        "encoder_input_data = pad_sequences([[word2idx.get(w, 0) for w in doc] for doc in tokenized_docs],\n",
        "                                   maxlen=max_input_len, padding='post')\n",
        "\n",
        "decoder_input_data = pad_sequences([[word2idx.get(w, 0) for w in sumry[:-1]] for sumry in tokenized_summaries],\n",
        "                                   maxlen=max_summary_len, padding='post')\n",
        "\n",
        "decoder_target_data = pad_sequences([[word2idx.get(w, 0) for w in sumry[1:]] for sumry in tokenized_summaries],\n",
        "                                    maxlen=max_summary_len, padding='post')\n",
        "\n",
        "\n",
        "# Train the model\n",
        "model.fit([encoder_input_data, decoder_input_data],\n",
        "          decoder_target_data,\n",
        "          batch_size=8,\n",
        "          epochs=20,\n",
        "          validation_split=0.2)\n",
        "#model was already runned for 20 epochs and i have stopped the training and saved the model see below"
      ],
      "metadata": {
        "id": "EM1g8so0yRcu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "f42707a0-ebab-4ba5-b72b-281c49128aad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m  3/300\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18:50\u001b[0m 4s/step - accuracy: 0.7180 - loss: 1.5743"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-909c18a07295>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m model.fit([encoder_input_data, decoder_input_data],\n\u001b[0m\u001b[1;32m     18\u001b[0m           \u001b[0mdecoder_target_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1683\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1684\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('summarization_model.h5')\n"
      ],
      "metadata": {
        "id": "GZDg9kZxBkuU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0086de80-d6b1-4f01-c33b-40a50e301253"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "test_data = dataset['test'].select(range(500))\n",
        "test_dataset=test_data.to_pandas()\n",
        "test_articles=test_dataset['article']\n",
        "test_summaries=test_dataset['highlights']\n",
        "\n",
        "# 1. Tokenize test articles and summaries\n",
        "X_test_tokens = [word_tokenize(doc.lower()) for doc in test_articles]\n",
        "y_test_tokens = [word_tokenize(sumry.lower()) for sumry in test_summaries]\n",
        "\n",
        "# 2. Convert tokens to sequences using your existing word_index\n",
        "\n",
        "\n",
        "def tokens_to_indices(token_list, word2idx, oov_token='unk'):\n",
        "    return [word2idx.get(token, word2idx.get(oov_token, 1)) for token in token_list]\n",
        "\n",
        "X_test_sequences = [tokens_to_indices(tokens, word2idx) for tokens in X_test_tokens]\n",
        "y_test_sequences = [tokens_to_indices(tokens, word2idx) for tokens in y_test_tokens]\n",
        "\n",
        "\n",
        "# 3. Pad sequences (use same maxlen as training)\n",
        "X_test_padded = pad_sequences(X_test_sequences, maxlen=400, padding='post', truncating='post')\n",
        "y_test_padded = pad_sequences(y_test_sequences, maxlen=101, padding='post', truncating='post')\n",
        "\n",
        "# 4. Load the model\n",
        "from tensorflow.keras.models import load_model\n",
        "model = load_model('summarization_model.h5')\n",
        "\n",
        "\n",
        "# Now this works\n",
        "decoder_input_test = y_test_padded[:, :-1]  # (None, 100)\n",
        "decoder_target_test = y_test_padded[:, 1:]  # (None, 100)\n",
        "\n",
        "loss, accuracy = model.evaluate([X_test_padded, decoder_input_test], decoder_target_test, verbose=1)\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy:.4f}, Test Loss: {loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HI1x2rQ32Imp",
        "outputId": "3f4d53e2-d4d5-43dd-e552-6ee9502340f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 1s/step - accuracy: 0.6550 - loss: 2.3211\n",
            "Test Accuracy: 0.6557, Test Loss: 2.3196\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge-score"
      ],
      "metadata": {
        "id": "XBmwg8kR5gi6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46721f07-48aa-414c-beaf-4d3497e90f17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (4.67.1)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=47bd1497ce07fa26c1745e46bd63159092c2a6003342296961d7647ca5171d6f\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "39J5h291hLwM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}